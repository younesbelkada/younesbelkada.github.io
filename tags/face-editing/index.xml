<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Face editing - Tag - Younes Belkada's blog</title><link>http://example.org/tags/face-editing/</link><description>Face editing - Tag - Younes Belkada's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>younesbelkada@gmail.com (Younes Belkada)</managingEditor><webMaster>younesbelkada@gmail.com (Younes Belkada)</webMaster><lastBuildDate>Sat, 12 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://example.org/tags/face-editing/" rel="self" type="application/rss+xml"/><item><title>InterfaceGAN++: How far can we go with InterfaceGAN?</title><link>http://example.org/posts/interface-gan/</link><pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate><author>younesbelkada@gmail.com (Younes Belkada)</author><guid>http://example.org/posts/interface-gan/</guid><description>&lt;div class="featured-image">
&lt;img src="/images/posts/interfacegan.gif" referrerpolicy="no-referrer">
&lt;/div>I would like to explain in this post, one the projects I was involved during my Masters degree at ENS Paris Saclay, during the Introduction to Numerical Imaging course.
Introduction InterfaceGAN is a paper that has been published on CVPR 2020, by Yujin Shen et al. It argues that well-trained generative models learns a disentangled latent space representation.
Basically given a generated face image from a random gaussian noise, InterfaceGAN can control some specific attributes of the face without altering the semantic information of it.</description></item></channel></rss>