<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Course notes - Category - Younes Belkada's blog</title><link>http://example.org/categories/course-notes/</link><description>Course notes - Category - Younes Belkada's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>younesbelkada@gmail.com (Younes Belkada)</managingEditor><webMaster>younesbelkada@gmail.com (Younes Belkada)</webMaster><lastBuildDate>Tue, 26 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="http://example.org/categories/course-notes/" rel="self" type="application/rss+xml"/><item><title>Week 4 - RL Course</title><link>http://example.org/notes/rl4/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><author>younesbelkada@gmail.com (Younes Belkada)</author><guid>http://example.org/notes/rl4/</guid><description>Model-free Reinforcement Learning We previously saw various cases of how to solve an MDP when the MDP is given (i.e. known transition probabilities and reward function). Often the problems are not that simple, as we call them model-free. The agent will only have access to a black box interacting with him.
We saw in this case that we can solve the MDP using several techniques such as Monte Carlo learning, that looks at several trajectories and estimates the empirical Value Function, or TD(0) learning that bootstraps the reward function of the next states and computes the difference between the expect reward at the next state and the observed reward at the next state.</description></item><item><title>Week 3 - RL Course</title><link>http://example.org/notes/rl3/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><author>younesbelkada@gmail.com (Younes Belkada)</author><guid>http://example.org/notes/rl3/</guid><description>No notes for today since it was an interactive session!</description></item><item><title>Week 2 - RL Course</title><link>http://example.org/notes/rl2/</link><pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate><author>younesbelkada@gmail.com (Younes Belkada)</author><guid>http://example.org/notes/rl2/</guid><description>In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.
Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem.</description></item><item><title>Week 1 - RL Course</title><link>http://example.org/notes/rl1/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><author>younesbelkada@gmail.com (Younes Belkada)</author><guid>http://example.org/notes/rl1/</guid><description>Here is the first personal notes from the Reinforcement Learning course that I have decided to take at the MVA Master&amp;rsquo;s program. The course is quite theoretical and this is challenging for me since I have (almost) zero knowledge on Reinforcement Learning. I have decided to try to explain what I have understood in each lecture in order to assimilate the content of each session.
What is Reinforcement Learning? RL is a family of Machine Learning techniques in order to solve a task that is based on the current and previous states of an agent interacting with an environment.</description></item></channel></rss>