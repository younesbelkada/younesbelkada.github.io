<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Mastering Natural Language Processing - Words as vectors - Younes Belkada's blog</title><meta name=description content="Younes Belkada's blog"><meta property="og:title" content="Mastering Natural Language Processing - Words as vectors"><meta property="og:description" content="Why NLP is so exciting? Natural Language Processing is an application of AI and Deep Learning that allows machines and algorithms understand languages (Natural Language) in order to easily deal with any problems related to text (text classification, sentiment analysis, summarization, etc.). There is also a very large interest around NLP from big tech companies and investors as the potential applications of Deep Learning for NLP are becoming more and more impactful."><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/posts/nlp-1/"><meta property="og:image" content="http://example.org/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-16T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-16T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://example.org/logo.png"><meta name=twitter:title content="Mastering Natural Language Processing - Words as vectors"><meta name=twitter:description content="Why NLP is so exciting? Natural Language Processing is an application of AI and Deep Learning that allows machines and algorithms understand languages (Natural Language) in order to easily deal with any problems related to text (text classification, sentiment analysis, summarization, etc.). There is also a very large interest around NLP from big tech companies and investors as the potential applications of Deep Learning for NLP are becoming more and more impactful."><meta name=application-name content="Younes Belkada"><meta name=apple-mobile-web-app-title content="Younes Belkada"><meta name=theme-color content="#ffc40d"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=http://example.org/posts/nlp-1/><link rel=prev href=http://example.org/posts/looking/><link rel=next href=http://example.org/posts/interface-gan/><link rel=stylesheet href=/css/page.min.css><link rel=stylesheet href=/css/home.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Mastering Natural Language Processing - Words as vectors","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/example.org\/posts\/nlp-1\/"},"genre":"posts","keywords":"Journal, Word Embeddings, Deep Learning, NLP","wordcount":435,"url":"http:\/\/example.org\/posts\/nlp-1\/","datePublished":"2022-01-16T00:00:00+00:00","dateModified":"2022-01-16T00:00:00+00:00","publisher":{"@type":"Organization","name":"Younes Belkada"},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body data-header-desktop data-header-mobile><script>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/>About </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/notes/>Notes </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title>About</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/notes/ title>Notes</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><div class=menu-item><a href=javascript:void(0); class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single" data-toc=enable><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/nlp_1.png data-srcset="/images/posts/nlp_1.png, /images/posts/nlp_1.png 1.5x, /images/posts/nlp_1.png 2x" data-sizes=auto alt=/images/posts/nlp_1.png title=/images/posts/nlp_1.png></div><div class=single-card data-image=true><h2 class="single-title animated flipInX">Mastering Natural Language Processing - Words as vectors</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=" author" class=author><i class="fas fa-user-circle fa-fw"></i>Younes Belkada</a></span>&nbsp;<span class=post-category>published in <a href=/categories/posts/><i class="far fa-folder fa-fw"></i>Posts</a></span></div><div class=post-meta-line><span><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-01-16>2022-01-16</time></span>&nbsp;<span><i class="fas fa-pencil-alt fa-fw"></i>&nbsp;435 words</span>&nbsp;
<span><i class="far fa-clock fa-fw"></i>&nbsp;3 minutes</span>&nbsp;</div></div><hr><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#why-nlp-is-so-exciting>Why NLP is so exciting?</a></li><li><a href=#from-a-language-for-humans-to-a-language-for-machines>From a language for Humans to a language for machines</a><ul><li><a href=#one-hot-vectors>One hot vectors</a></li><li><a href=#learning-words-representation>Learning words representation</a></li><li><a href=#useful-links>Useful links</a></li></ul></li></ul></nav></div></div><div class=content id=content><h2 id=why-nlp-is-so-exciting>Why NLP is so exciting?</h2><p>Natural Language Processing is an application of AI and Deep Learning that allows machines and algorithms understand languages (Natural Language) in order to easily deal with any problems related to text (text classification, sentiment analysis, summarization, etc.). There is also a very large interest around NLP from big tech companies and investors as the potential applications of Deep Learning for NLP are becoming more and more impactful.</p><h2 id=from-a-language-for-humans-to-a-language-for-machines>From a language for Humans to a language for machines</h2><p>How do we teach machines to understand human language? Humans understand the each word that we read and listen because we know their meanings. A <strong>meaning</strong> can be defined as</p><ul><li>The idea that is represented by a word, phrase, etc.</li><li>The idea that a person wants to express by using words, signs, etc.</li><li>The idea that is expressed in a work of writing, art, etc.</li></ul><p>A machine needs to do the same thing. Associate to each word, a meaning. Previous work includes resources like <em>WordNet</em> that stores word&rsquo;s synonyms and semantics but can be showed that it is not <em>scalable</em>. We will see that there are several ways to explicitly teach a machine to understand and model a words meaning.</p><h3 id=one-hot-vectors>One hot vectors</h3><p>Here comes the simplest approach for converting any word into a vector. We can simply encode any word into a binary vector. Below is a dummy example:</p><figure><a class=lightgallery href=/images/posts/one-hot-vectors.png title=/images/posts/one-hot-vectors.png data-thumbnail=/images/posts/one-hot-vectors.png data-sub-html="<h2>A dummy example for one-hot encoding- Source: Stanford cs224n lecture</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/one-hot-vectors.png data-srcset="/images/posts/one-hot-vectors.png, /images/posts/one-hot-vectors.png 1.5x, /images/posts/one-hot-vectors.png 2x" data-sizes=auto alt=/images/posts/one-hot-vectors.png></a><figcaption class=image-caption>A dummy example for one-hot encoding- Source: Stanford cs224n lecture</figcaption></figure><div class="details admonition quote open"><div class="details-summary admonition-title"><i class="icon fas fa-quote-right fa-fw"></i>Quote<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content>These two vectors are not orthogonal therefore, there is no natural notion of <strong>similarity</strong> for one-hot vectors!</div></div></div><h3 id=learning-words-representation>Learning words representation</h3><p>A word can be also defined by its context, <em>i.e.</em>, the nearby words of it. This idea can be exploited, rather than representing each word independently, a word should be represented by its context and this is called <em>Distributional semantics</em>.<div class="details admonition quote open"><div class="details-summary admonition-title"><i class="icon fas fa-quote-right fa-fw"></i>Quote from J.R. Firth<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content><em>You shall know a word by the company it keeps</em></div></div></div>Therefore, the representation of the word <em>finance</em> should be close to the representation of the words <em>money</em>, <em>bank</em>, <em>trading</em>, <em>currency</em>, etc. This idea appeared to be very successful and used in techniques such as <em>Word2Vec</em>.</p><ul><li>At each position <em>t</em> on the text, we have the center word <em>o</em> and the surrounding context words <em>c</em>.</li><li>Objective: For each position <em>t</em> in the text, predict the context words with a window size of <em>m</em>, given the center word.</li><li>Objective function: Log-likelihood</li></ul><figure><a class=lightgallery href=/images/posts/word2vec.png title=/images/posts/word2vec.png data-thumbnail=/images/posts/word2vec.png data-sub-html="<h2>Training pipeline of Word2vec - Source: Stanford cs224n lecture</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/word2vec.png data-srcset="/images/posts/word2vec.png, /images/posts/word2vec.png 1.5x, /images/posts/word2vec.png 2x" data-sizes=auto alt=/images/posts/word2vec.png></a><figcaption class=image-caption>Training pipeline of Word2vec - Source: Stanford cs224n lecture</figcaption></figure><h3 id=useful-links>Useful links</h3><ul><li><a href=http://web.stanford.edu/class/cs224n/ target=_blank rel="noopener noreffer">Stanford CS224n lectures</a></li><li><a href=https://ai.stackexchange.com/questions/26739/what-is-the-difference-between-a-language-model-and-a-word-embedding target=_blank rel="noopener noreffer">Word2Vec vs Language Models</a></li><li><a href=https://people.cs.umass.edu/~miyyer/cs685/schedule.html target=_blank rel="noopener noreffer">UMass CS685</a></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-tag><span><a href=/tags/journal/>Journal</a>
</span><span><a href=/tags/word-embeddings/>Word Embeddings</a>
</span><span><a href=/tags/deep-learning/>Deep Learning</a>
</span><span><a href=/tags/nlp/>NLP</a></span></div><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-01-16</span></div><div class=post-info-mod></div></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=http://example.org/posts/nlp-1/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="Share on WhatsApp" data-sharer=whatsapp data-url=http://example.org/posts/nlp-1/ data-title="Mastering Natural Language Processing - Words as vectors" data-web><i class="fab fa-whatsapp fa-fw"></i></a></span></div></div><div class=post-nav><a href=/posts/looking/ class=prev rel=prev title="'Do pedestrians pay attention? Eye contact detection in the wild' on arxiv"><i class="fas fa-angle-left fa-fw"></i>Previous Post</a>
<a href=/posts/interface-gan/ class=next rel=next title="InterfaceGAN++: How far can we go with InterfaceGAN?">Next Post<i class="fas fa-angle-right fa-fw"></i></a></div></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.85.0">Hugo</a> | Theme - <a href=https://github.com/khusika/FeelIt target=_blank rel="noopener noreffer" title="FeelIt 1.0.1"><i class="fas fa-hand-holding-heart fa-fw"></i> FeelIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/>Younes Belkada</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-chevron-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment-alt fa-fw"></i></a></div><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><script src=/lib/lazysizes/lazysizes.min.js></script><script src=/lib/lightgallery/lightgallery.min.js></script><script src=/lib/lightgallery/lg-thumbnail.min.js></script><script src=/lib/lightgallery/lg-zoom.min.js></script><script src=/lib/clipboard/clipboard.min.js></script><script src=/lib/sharer/sharer.min.js></script><script src=/lib/katex/katex.min.js></script><script src=/lib/katex/auto-render.min.js></script><script src=/lib/katex/copy-tex.min.js></script><script src=/lib/katex/mhchem.min.js></script><script>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script src=/js/theme.min.js></script></body></html>