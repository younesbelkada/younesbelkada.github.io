<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Imitation Learning explained - Younes Belkada's blog</title><meta name=description content="Younes Belkada's blog"><meta property="og:title" content="Imitation Learning explained"><meta property="og:description" content="Here is my attempt to explain the notions and intuitions behind Imitation Learning with the best of my knowledge. Credits to this very nice blog where I have learned most of the things that I have understood about the concept, and to this website for the image above. Now let&rsquo;s directly dive in.
1. Brief intuitions In Reinforcement Learning, you learn to make good sequence of decisionsSource: http://web.stanford.edu/class/cs234/slides/lecture1.pdf
&#34; In Reinforcement Learning, you learn to make good sequence of decisions  The field of Reinforcement Learning is an area of machine learning where an intelligent agent interacts with an environment in order to learn a policy (i."><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/posts/imitation_learning/"><meta property="og:image" content="http://example.org/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-10-14T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-14T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://example.org/logo.png"><meta name=twitter:title content="Imitation Learning explained"><meta name=twitter:description content="Here is my attempt to explain the notions and intuitions behind Imitation Learning with the best of my knowledge. Credits to this very nice blog where I have learned most of the things that I have understood about the concept, and to this website for the image above. Now let&rsquo;s directly dive in.
1. Brief intuitions In Reinforcement Learning, you learn to make good sequence of decisionsSource: http://web.stanford.edu/class/cs234/slides/lecture1.pdf
&#34; In Reinforcement Learning, you learn to make good sequence of decisions  The field of Reinforcement Learning is an area of machine learning where an intelligent agent interacts with an environment in order to learn a policy (i."><meta name=application-name content="Younes Belkada"><meta name=apple-mobile-web-app-title content="Younes Belkada"><meta name=theme-color content="#ffc40d"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=http://example.org/posts/imitation_learning/><link rel=prev href=http://example.org/posts/brain_tumor/><link rel=stylesheet href=/css/page.min.css><link rel=stylesheet href=/css/home.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Imitation Learning explained","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/example.org\/posts\/imitation_learning\/"},"genre":"posts","keywords":"Explanation, Reinforcement Learning, Deep Learning, Imitation Learning","wordcount":893,"url":"http:\/\/example.org\/posts\/imitation_learning\/","datePublished":"2021-10-14T00:00:00+00:00","dateModified":"2021-10-14T00:00:00+00:00","publisher":{"@type":"Organization","name":"Younes Belkada"},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body data-header-desktop data-header-mobile><script>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/>About </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/notes/>Notes </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title>About</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/notes/ title>Notes</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><div class=menu-item><a href=javascript:void(0); class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single" data-toc=enable><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/robot-reading.png data-srcset="/images/posts/robot-reading.png, /images/posts/robot-reading.png 1.5x, /images/posts/robot-reading.png 2x" data-sizes=auto alt=/images/posts/robot-reading.png title=/images/posts/robot-reading.png></div><div class=single-card data-image=true><h2 class="single-title animated flipInX">Imitation Learning explained</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=" author" class=author><i class="fas fa-user-circle fa-fw"></i>Younes Belkada</a></span>&nbsp;<span class=post-category>published in <a href=/categories/posts/><i class="far fa-folder fa-fw"></i>Posts</a></span></div><div class=post-meta-line><span><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-10-14>2021-10-14</time></span>&nbsp;<span><i class="fas fa-pencil-alt fa-fw"></i>&nbsp;893 words</span>&nbsp;
<span><i class="far fa-clock fa-fw"></i>&nbsp;5 minutes</span>&nbsp;</div></div><hr><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#1-brief-intuitions>1. Brief intuitions</a><ul><li><a href=#11-how-to-find-the-best-policy-in-rl>1.1 How to find the best policy in RL?</a></li><li><a href=#12-why-imitation-learning>1.2 Why Imitation Learning?</a></li><li><a href=#13-different-types-of-algorithms>1.3 Different types of algorithms</a></li></ul></li><li><a href=#2-imitation-learning-in-practice>2. Imitation learning in practice</a><ul><li><a href=#21-behavioural-cloning>2.1 Behavioural Cloning</a></li><li><a href=#22-towards-more-accurate-training-data>2.2 Towards more accurate training data</a></li></ul></li></ul></li></ul></nav></div></div><div class=content id=content><p>Here is my attempt to explain the notions and intuitions behind <em>Imitation Learning</em> with the best of my knowledge. Credits to this very nice <a href=https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c target=_blank rel="noopener noreffer">blog</a> where I have learned most of the things that I have understood about the concept, and to this <a href=https://www.stateofdigital.com/we-need-to-write-for-robots-until-they-read-like-people-why-our-industry-needs-technical-content-creators/ target=_blank rel="noopener noreffer">website</a> for the image above. Now let&rsquo;s directly dive in.</p><h3 id=1-brief-intuitions>1. Brief intuitions</h3><figure><a class=lightgallery href=/images/posts/imitation_learning_1.png title="Source: http://web.stanford.edu/class/cs234/slides/lecture1.pdf" data-thumbnail=/images/posts/imitation_learning_1.png data-sub-html="<h2>In Reinforcement Learning, you learn to make good sequence of decisions</h2><p>Source: http://web.stanford.edu/class/cs234/slides/lecture1.pdf</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/imitation_learning_1.png data-srcset="/images/posts/imitation_learning_1.png, /images/posts/imitation_learning_1.png 1.5x, /images/posts/imitation_learning_1.png 2x" data-sizes=auto alt=/images/posts/imitation_learning_1.png height=400 width=400></a><figcaption class=image-caption>In Reinforcement Learning, you learn to make good sequence of decisions</figcaption></figure><p>The field of Reinforcement Learning is an area of machine learning where an <em>intelligent</em> agent interacts with an environment in order to learn a policy (i.e. a set of <em>rules</em>) that maximizes the rewards given by the environment. More details can be found on the <a href=https://younesbelkada.github.io/notes/rl1/ target=_blank rel="noopener noreffer">course notes</a> that I have took on the Reinforcement Learning class that I am currently following.</p><h4 id=11-how-to-find-the-best-policy-in-rl>1.1 How to find the best policy in RL?</h4><p>There are several algorithms and learning methods in order to <em>learn</em> the best <em>policy</em> in RL. I will not explain in detail all the possible ways to learn the best policy in RL, but the tricky part there is to design the <strong>reward function</strong>. In the typical and basic scenario, the reward function are designed manually due to the simplicity of the problem (e.g. a simple Atari game).</p><h4 id=12-why-imitation-learning>1.2 Why Imitation Learning?</h4><p>This can be extremely complex in some scenarios (e.g. self-driving cars) where the environment can get extremely complex, thus the reward function really hard to design manually. Here comes <em>Imitation Learning</em> (IL) to solve this issue. I am quoting below a very clear explanation of IL from the <a href=https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c target=_blank rel="noopener noreffer">blog</a> I am referring.</p><div class="details admonition quote open"><div class="details-summary admonition-title"><i class="icon fas fa-quote-right fa-fw"></i>Quote from the blog<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content>In IL instead of trying to learn from the sparse rewards or manually specifying a reward function, an expert (typically a human) provides us with a set of demonstrations.</div></div></div><p>It is now very obvious why <em>Imitation Learning</em> is called so. An agent learns by imitating an expert that shows the <em>correct</em> behavior on the environment.</p><h4 id=13-different-types-of-algorithms>1.3 Different types of algorithms</h4><p>Several types of algorithms exists for <em>IL</em> from the simplest to the less simple: <em>Behavioural Cloning</em>, <em>Direct Policy Learning via Interactive Demonstrator</em> and <em>Inverse Reinforcement Learning</em>. Let&rsquo;s quickly focus on the Behavioural Cloning that works as follows:</p><ul><li>The agent collects the demonstrations (trajectories) from the expert</li><li>Treat the demonstrations as i.i.d state-action pairs</li><li>Learn the optimal policy using supervised learning</li></ul><p>In practice, this algorithm is not that robust. This is due to the fact that the state-actions paris are treated as i.i.d (every pair is independent from each other).</p><h3 id=2-imitation-learning-in-practice>2. Imitation learning in practice</h3><p>Let us consider that we are evolving in an environment where we get a reward at each state and we have a predefined finite set of actions. Let us also assume that it is <strong>really hard</strong> to design an accurate reward function due to the various possibility of sets.</p><figure><a class=lightgallery href=/images/posts/il_scheme.png title="Original Content" data-thumbnail=/images/posts/il_scheme.png data-sub-html="<h2>In Reinforcement Learning, you learn to make good sequence of decisions</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/il_scheme.png data-srcset="/images/posts/il_scheme.png, /images/posts/il_scheme.png 1.5x, /images/posts/il_scheme.png 2x" data-sizes=auto alt=/images/posts/il_scheme.png height=1000 width=1000></a><figcaption class=image-caption>In Reinforcement Learning, you learn to make good sequence of decisions</figcaption></figure><h4 id=21-behavioural-cloning>2.1 Behavioural Cloning</h4><p>As described in this example above (Original content), the reward function is really hard to design manually as the environment is extremely stochastic, i.e. the lion&rsquo;s behavior is really unpredictable.</p><p>To tackle this issue, the <em>IL</em> aims at collecting a training data from an <em>expert</em> beforehand. In our scenario, we can imagine that a lion expert will demonstrate how to deal with the environment and we will collect all the records of the scenarios that happened to build a training dataset.</p><figure><a class=lightgallery href=/images/posts/analogy_il.png title="Original Content" data-thumbnail=/images/posts/analogy_il.png data-sub-html="<h2>The expert demonstrates how to deal with the environment</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/analogy_il.png data-srcset="/images/posts/analogy_il.png, /images/posts/analogy_il.png 1.5x, /images/posts/analogy_il.png 2x" data-sizes=auto alt=/images/posts/analogy_il.png height=1000 width=1000></a><figcaption class=image-caption>The expert demonstrates how to deal with the environment</figcaption></figure><p>After getting the training data, we can learn an optimal <strong>policy</strong> by doing supervised learning on the obtain pairs of states and actions.</p><p>This typical scenario is called <strong>Behavioural Cloning</strong> as explained before. But this has some limitations:</p><ul><li>The collected training data is considered to be a set of i.i.d samples (each state is independent between each other). Which is in practice, not the case.</li><li><em>What if the agent follows a set of paths that the expert did not encountered?</em> This can easily lead to exploring unknown states, and to lead to catastrophic failures</li></ul><h4 id=22-towards-more-accurate-training-data>2.2 Towards more accurate training data</h4><p>What if we can access to our expert&rsquo;s feedbacks in real time while training our model? That is the main idea behind <em>Direct Policy Learning via Interactive Demonstrator</em>. Let us assume we already have in our bag the training data collected from our lion&rsquo;s expert.</p><figure><a class=lightgallery href=/images/posts/irdl.png title="Original Content" data-thumbnail=/images/posts/irdl.png data-sub-html="<h2>Direct Policy Learning pipeline</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/irdl.png data-srcset="/images/posts/irdl.png, /images/posts/irdl.png 1.5x, /images/posts/irdl.png 2x" data-sizes=auto alt=/images/posts/irdl.png height=400 width=400></a><figcaption class=image-caption>Direct Policy Learning pipeline</figcaption></figure><p>Basilcally from the initial training data:</p><ol><li>We train our agent on it in order to get the first optimal policy</li><li>We run the policy on the environement in order to get the new observations</li><li>Ask our expert to label the new observations (feedbacks)</li><li>Combine both datasets and repeat the steps <em>1-4</em> until convergence</li></ol><figure><a class=lightgallery href=/images/posts/analogy_il_2.png title="Original Content" data-thumbnail=/images/posts/analogy_il_2.png data-sub-html="<h2>The expert augments the initial dataset until convergence with his feedbacks</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/analogy_il_2.png data-srcset="/images/posts/analogy_il_2.png, /images/posts/analogy_il_2.png 1.5x, /images/posts/analogy_il_2.png 2x" data-sizes=auto alt=/images/posts/analogy_il_2.png height=1000 width=1000></a><figcaption class=image-caption>The expert augments the initial dataset until convergence with his feedbacks</figcaption></figure><p>By applying this technique, the agent will not suffer from the issues faced with Behavioural Cloning. But this has a price, it is really hard to have a real-time feedback from an expert (a human) while training the model. Also in some cases the concept of expert can be hard to define.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-tag><span><a href=/tags/explanation/>Explanation</a>
</span><span><a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
</span><span><a href=/tags/deep-learning/>Deep Learning</a>
</span><span><a href=/tags/imitation-learning/>Imitation Learning</a></span></div><div class=post-info-line><div class=post-info-mod><span>Updated on 2021-10-14</span></div><div class=post-info-mod></div></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=http://example.org/posts/imitation_learning/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="Share on WhatsApp" data-sharer=whatsapp data-url=http://example.org/posts/imitation_learning/ data-title="Imitation Learning explained" data-web><i class="fab fa-whatsapp fa-fw"></i></a></span></div></div><div class=post-nav><a href=/posts/brain_tumor/ class=prev rel=prev title="Challenge 1 - Brain Tumor classification on Kaggle"><i class="fas fa-angle-left fa-fw"></i>Previous Post</a></div></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.85.0">Hugo</a> | Theme - <a href=https://github.com/khusika/FeelIt target=_blank rel="noopener noreffer" title="FeelIt 1.0.1"><i class="fas fa-hand-holding-heart fa-fw"></i> FeelIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/>Younes Belkada</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-chevron-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment-alt fa-fw"></i></a></div><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><script src=/lib/lazysizes/lazysizes.min.js></script><script src=/lib/lightgallery/lightgallery.min.js></script><script src=/lib/lightgallery/lg-thumbnail.min.js></script><script src=/lib/lightgallery/lg-zoom.min.js></script><script src=/lib/clipboard/clipboard.min.js></script><script src=/lib/sharer/sharer.min.js></script><script src=/lib/katex/katex.min.js></script><script src=/lib/katex/auto-render.min.js></script><script src=/lib/katex/copy-tex.min.js></script><script src=/lib/katex/mhchem.min.js></script><script>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script src=/js/theme.min.js></script></body></html>