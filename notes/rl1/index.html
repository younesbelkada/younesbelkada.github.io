<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Week 1 - RL Course - Younes Belkada's blog</title><meta name=description content="Younes Belkada's blog"><meta property="og:title" content="Week 1 - RL Course"><meta property="og:description" content="Here is the first personal notes from the Reinforcement Learning course that I have decided to take at the MVA Master&rsquo;s program. The course is quite theoretical and this is challenging for me since I have (almost) zero knowledge on Reinforcement Learning. I have decided to try to explain what I have understood in each lecture in order to assimilate the content of each session.
What is Reinforcement Learning? RL is a family of Machine Learning techniques in order to solve a task that is based on the current and previous states of an agent interacting with an environment."><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/notes/rl1/"><meta property="og:image" content="http://example.org/logo.png"><meta property="article:section" content="notes"><meta property="article:published_time" content="2021-10-05T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-05T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://example.org/logo.png"><meta name=twitter:title content="Week 1 - RL Course"><meta name=twitter:description content="Here is the first personal notes from the Reinforcement Learning course that I have decided to take at the MVA Master&rsquo;s program. The course is quite theoretical and this is challenging for me since I have (almost) zero knowledge on Reinforcement Learning. I have decided to try to explain what I have understood in each lecture in order to assimilate the content of each session.
What is Reinforcement Learning? RL is a family of Machine Learning techniques in order to solve a task that is based on the current and previous states of an agent interacting with an environment."><meta name=application-name content="Younes Belkada"><meta name=apple-mobile-web-app-title content="Younes Belkada"><meta name=theme-color content="#ffc40d"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=http://example.org/notes/rl1/><link rel=next href=http://example.org/notes/rl2/><link rel=stylesheet href=/css/page.min.css><link rel=stylesheet href=/css/home.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Week 1 - RL Course","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/example.org\/notes\/rl1\/"},"genre":"notes","keywords":"Reinforcement Learning, Maths, Markov Chains","wordcount":1103,"url":"http:\/\/example.org\/notes\/rl1\/","datePublished":"2021-10-05T00:00:00+00:00","dateModified":"2021-10-05T00:00:00+00:00","publisher":{"@type":"Organization","name":"Younes Belkada"},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body data-header-desktop data-header-mobile><script>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/>About </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/notes/>Notes </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title>About</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/notes/ title>Notes</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><div class=menu-item><a href=javascript:void(0); class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><main class=main><div class=container><div class=toc id=toc-auto style=top:8rem><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single special" data-toc=enable><h2 class="single-title animated fadeInDown faster">Week 1 - RL Course</h2><div class=single-card><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#probelm-statement-in-rl>Probelm statement in RL</a></li><li><a href=#markov-chains-and-rl>Markov Chains and RL</a></li><li><a href=#granularity-of-the-time-step>Granularity of the time step</a></li><li><a href=#assumptions-under-a-markov-decision-process>Assumptions under a Markov Decision Process</a></li></ul></li></ul><ul><li><ul><li><a href=#if-the-time-horizon-is-finite>If the time horizon is finite</a></li><li><a href=#if-the-time-horizon-is-infinite>If the time horizon is infinite</a></li><li><a href=#optimal-value-function>Optimal Value Function</a></li><li><a href=#q-function>Q-function</a></li></ul></li></ul><ul><li><ul><li><a href=#notations>Notations</a></li><li><a href=#bellman-equation>Bellman equation</a></li><li><a href=#bellman-operator>Bellman operator</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>Here is the first personal notes from the <em>Reinforcement Learning</em> course that I have decided to take at the <em>MVA</em> Master&rsquo;s program. The course is quite theoretical and this is challenging for me since I have (almost) zero knowledge on Reinforcement Learning. I have decided to try to explain what I have understood in each lecture in order to assimilate the content of each session.</p><h1 id=what-is-reinforcement-learning>What is Reinforcement Learning?</h1><p>RL is a family of Machine Learning techniques in order to <strong>solve a task</strong> that is based on the <strong>current</strong> and <strong>previous states</strong> of an agent interacting with an environment. In the most practical cases, the agent learns by interacting under uncertainty (i.e. the environment is <strong>unknown</strong>).
In practice, RL can solve several problems related to portfolio management, autonomous driving, game theory and so on.</p><h3 id=probelm-statement-in-rl>Probelm statement in RL</h3><p>In a RL configuration the time needs to be discrete, i.e $t = 1, 2, 3$ thus not continuous. At each time stepm the agent:</p><ul><li>Selects and action based on the current state or on all the states</li><li>Gets a reward</li><li>moves to a new state</li></ul><figure><a class=lightgallery href=/images/notes/rl1.png title=/images/notes/rl1.png data-thumbnail=/images/notes/rl1.png data-sub-html="<h2>Reinforcement Learning</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/rl1.png data-srcset="/images/notes/rl1.png, /images/notes/rl1.png 1.5x, /images/notes/rl1.png 2x" data-sizes=auto alt=/images/notes/rl1.png height=400 width=400></a><figcaption class=image-caption>Reinforcement Learning</figcaption></figure><h3 id=markov-chains-and-rl>Markov Chains and RL</h3><p>A Markov chain is a dynamic system that satifies the <strong>Markov Property</strong>:</p><p>$$
\begin{equation}
\mathbb{P}(s_{t+1} | s_{t}, s_{t-1},&mldr;,s_{0}) = \mathbb{P}(s_{t+1} | s_{t})
\end{equation}
$$</p><p>Also, the transition between each state is determined by a probability called <strong>transition probability</strong>. In the case of RL, we also consider the space of actions $A$, the transition probability formula becomes:</p><p>$$
\begin{equation}
p(s'|s, a) = \mathbb{P}(s_{t+1} | s_{t}=s, a_{t} = a)
\end{equation}
$$</p><h3 id=granularity-of-the-time-step>Granularity of the time step</h3><p>How granular needs to be the time step? The granularity will depend on each problem and needs to be <strong>fine-tuned</strong>. For e.g., for an Atari game we may need 4 frames between each step in order to notice the direction of the ball to take the optimal decision</p><h3 id=assumptions-under-a-markov-decision-process>Assumptions under a Markov Decision Process</h3><ul><li>Each transition needs to be characterized by a reward that can be <strong>deterministic</strong> or <strong>stochastic</strong>.</li><li>The dynamics and reward <strong>do not</strong> change over time</li></ul><h1 id=policies-and-decision-rules>Policies and Decision rules</h1><p>In order to solve a MDP and find the optimal solution, the final goal for the agents is to learn a <strong>Policy</strong> that is defined as a <strong>set of decision rule</strong>. The latest, is a function $d: S \mapsto A$ that takes a state $s \in S$ as an input, and outputs and action $a \in A$.
The decision rule can be:</p><ul><li>Deterministic</li><li>Stochastic</li><li>History-dependent</li><li>Markov?</li></ul><p>An agent behaving under the policy $\pi$ selects at round $t$ the action:
$$a_{t} \backsim d_{t}(s_{t})$$</p><h1 id=optimiality-principles>Optimiality Principles</h1><p>A metric to &lsquo;evaluate&rsquo; a policy is to compute the <strong>State Value Function</strong> $V^{\pi}$.</p><h3 id=if-the-time-horizon-is-finite>If the time horizon is finite</h3><p>$$V^{\pi}(t, s) = \mathbb{E}[ \sum_{\tau = t}^{T-1} r(s_{\tau}, d_{\tau} (s_{\tau})) + R(s_{T}) | s_{t} = s; \pi = (d_1,&mldr;,d_{T})]$$</p><h3 id=if-the-time-horizon-is-infinite>If the time horizon is infinite</h3><p>This is used in practice when there is an uncertainty about the deadline.</p><p>$$V^{\pi}(s) = \mathbb{E}[ \sum_{t=0}^{\infty} \gamma^t r(s_{t}, d_{t} (s_{t})) | s_{0} = s; \pi = (d_1,&mldr;,d_{T})]$$ With $$\gamma \in [0,1)$$</p><p>Basically the coefficient $\gamma$ indicates around which time step we should get most of the rewards. If $\gamma$ is small, then the STF will focus on <em>short-term rewards</em>, otherwise the function will flag <em>long-term rewards</em>.</p><h3 id=optimal-value-function>Optimal Value Function</h3><p>The solution to an MDP is an optimal policy satisfying</p><p>$$\pi^{*} \in argmax V^{\pi}_{\pi \in \Pi}$$</p><p>It is not an equality since we may have several optimal solutions for the problem. The corresponding value function is the optimal value function $$ V^{*} = V^{\pi^{*}} $$</p><h3 id=q-function>Q-function</h3><p>The Q-function is a function that maps from $ S \times A \mapsto \mathbb{R} $</p><p>$$
Q^{\pi} (s, a) = \mathbb{E} [ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) | s_0 = 0, a_0 = a, a_t = \pi(s_t) ]
$$</p><p>The Q-function represents the expected reward when we take an action $a$ in the state $s$. It represents the <strong>quality</strong> of a certain action in a given state.</p><h1 id=bellman-operator-and-bellman-equation>Bellman operator and Bellman equation</h1><h3 id=notations>Notations</h3><p>An example is worth thousands words. In order to define the variables that we are going to use, let&rsquo;s define a simple MDP which is the one below:</p><figure><a class=lightgallery href=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png title=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-thumbnail=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-sub-html="<h2>A simple MDP</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-srcset="https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png, https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png 1.5x, https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png 2x" data-sizes=auto alt=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png height=400 width=400></a><figcaption class=image-caption>A simple MDP</figcaption></figure><p>The decision rule $\pi$ is a set of size $N$, with $N$ corresponding to the number of states. Each value of $\pi_i$ corresponds to the action that we take at the state $s_i$.
$$\pi = \{\textcolor{#E94D40}{\pi_0},\ &mldr;\ ,\ \pi_N \} $$
$$ \textcolor{#E94D40}{\pi_{0\ }}\ \in\ A $$</p><p>The transition probabilities matrix can be easily written. Each row $i$ corresponds to the probabilities of moving from a state $i$ to all possible states $j \in {1,..,n}$ <strong>given a decision rule $\pi$</strong>. Thus here if $\pi$ is stationary and $\pi = \{\textcolor{#E94D40}{a_0},\ \textcolor{#E94D40}{a_0} ,\ \textcolor{#3697DC}{a_1}\}$,</p><p>$$ \begin{align} P = \begin{pmatrix} 0 & 0.1 & 0.9 \\\ 1 & 0 & 0 \\\ 0 & 0 & 1 \end{pmatrix} \end{align} $$</p><p>The reward vector $r^\pi$ is the vector containing the <em><strong>expected</strong></em> rewards at each state, given a policy $\pi$. In our case, using the same policy, we obtain</p><p>$$ \begin{align} r^\pi & = & \begin{pmatrix} 0 \\\ \mathbb{E}(Be(1/3)) \\\ 0 \end{pmatrix} & = & \begin{pmatrix} 0 \\\ 1/3 \\\ 0 \end{pmatrix} \end{align} $$</p><p>If $\pi$ was $\{\textcolor{#E94D40}{a_0},\ \textcolor{#E94D40}{a_0} ,\ \textcolor{#E94D40}{a_0}\}$ we would obtain:</p><p>$$ \begin{align} r^\pi & = & \begin{pmatrix} 0 \\\ \mathbb{E}(Be(1/3)) \\\ \mathbb{E}(Be(2/3)) \end{pmatrix} & = & \begin{pmatrix} 0 \\\ 1/3 \\\ 2/3 \end{pmatrix} \end{align} $$</p><h3 id=bellman-equation>Bellman equation</h3><p>Given a state $s$ and, policy $\pi$ and a set of transition probability, we can analytically compute the value of the Value Function in an exact way using the Bellman Equation.</p><p>$$ \begin{equation} V^{\pi}(s)=r(s, \pi(s))+\gamma \sum_{y} p(y \mid s, \pi(s)) V^{\pi}(y) \end{equation} $$</p><p>The equation below can be written also in a matrix form:</p><p>$$ \begin{align} V^{\pi} &=r^{\pi}+\gamma P^{\pi} V^{\pi} & & \Longrightarrow & V^{\pi} &=\left(I-\gamma P^{\pi}\right)^{-1} r^{\pi} \end{align} $$</p><p>which can be easily computed if <em><strong>all the variable were know beforehand,</strong></em> which in practice not really the case since the environment with which the agent interacts is unknown.</p><h3 id=bellman-operator>Bellman operator</h3><p>For any $W \in \mathbb{R}^n$, the Bellman operator is a function $\mathcal{T}^{\pi} : \mathbb{R}^n \mapsto \mathbb{R}^n$ such that:</p><p>$
\mathcal{T}^{\pi} W(s)=r(s, \pi(s))+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, \pi(s)\right) W\left(s^{\prime}\right)
$</p><p>This operator encapsulates the fundamental operation in order to compute a Value Function, given a MDP</p><p>When the policy is <em><strong>unknown</strong></em> we use the following formula to compute each component of $W$:
$
\mathcal{T} W(s)=\max _{a \in A}\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) W(s)\right]
$</p><p>As you can see, the formula of the Bellman equation is<em>recursive</em>. In practice, this is computed and solved using <em>dynamic programming</em>.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-tag><span><a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
</span><span><a href=/tags/maths/>Maths</a>
</span><span><a href=/tags/markov-chains/>Markov Chains</a></span></div><div class=post-info-line><div class=post-info-mod><span>Updated on 2021-10-05</span></div><div class=post-info-mod></div></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=http://example.org/notes/rl1/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="Share on WhatsApp" data-sharer=whatsapp data-url=http://example.org/notes/rl1/ data-title="Week 1 - RL Course" data-web><i class="fab fa-whatsapp fa-fw"></i></a></span></div></div><div class=post-nav><a href=/notes/rl2/ class=next rel=next title="Week 2 - RL Course">Next Post<i class="fas fa-angle-right fa-fw"></i></a></div></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.85.0">Hugo</a> | Theme - <a href=https://github.com/khusika/FeelIt target=_blank rel="noopener noreffer" title="FeelIt 1.0.1"><i class="fas fa-hand-holding-heart fa-fw"></i> FeelIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/>Younes Belkada</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-chevron-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment-alt fa-fw"></i></a></div><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><script src=/lib/lazysizes/lazysizes.min.js></script><script src=/lib/lightgallery/lightgallery.min.js></script><script src=/lib/lightgallery/lg-thumbnail.min.js></script><script src=/lib/lightgallery/lg-zoom.min.js></script><script src=/lib/clipboard/clipboard.min.js></script><script src=/lib/sharer/sharer.min.js></script><script src=/lib/katex/katex.min.js></script><script src=/lib/katex/auto-render.min.js></script><script src=/lib/katex/copy-tex.min.js></script><script src=/lib/katex/mhchem.min.js></script><script>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script src=/js/theme.min.js></script></body></html>