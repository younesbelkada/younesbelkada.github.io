<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><link rel=next href=https://younesbelkada.github.io/notes/rl2/><link rel=canonical href=https://younesbelkada.github.io/notes/rl1/><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title>Week 1 - RL Course | Younes Belkada</title><meta name=title content="Week 1 - RL Course | Younes Belkada"><link rel=stylesheet href=/font/iconfont.css><link rel=stylesheet href=/css/main.min.css><meta name=twitter:card content="summary"><meta name=twitter:title content="Week 1 - RL Course"><meta name=twitter:description content="Here is the first personal notes from the Reinforcement Learning course that I have decided to take at the MVA Master&rsquo;s program. The course is quite theoretical and this is challenging for me since I have (almost) zero knowledge on Reinforcement Learning. I have decided to try to explain what I have understood in each lecture in order to assimilate the content of each session.
What is Reinforcement Learning? RL is a family of Machine Learning techniques in order to solve a task that is based on the current and previous states of an agent interacting with an environement."><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Week 1 - RL Course","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/younesbelkada.github.io\/notes\/rl1\/"},"image":{"@type":"ImageObject","url":"https:\/\/younesbelkada.github.io\/cover.png","width":800,"height":600},"genre":"notes","wordcount":1003,"url":"https:\/\/younesbelkada.github.io\/notes\/rl1\/","datePublished":"2021-10-05T00:00:00\u002b00:00","dateModified":"2021-10-05T00:00:00\u002b00:00","publisher":{"@type":"Organization","name":"Fastbyte01","logo":{"@type":"ImageObject","url":"https:\/\/younesbelkada.github.io\/logo.png","width":127,"height":40}},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body><div class=wrapper><nav class=navbar><div class=container><div class="navbar-header header-logo"><a href=https://younesbelkada.github.io/>Younes Belkada</a></div><div class="menu navbar-right"><a class=menu-item href=/posts/ title>Blog</a>
<a class=menu-item href=/notes/ title>Notes</a>
<a class=menu-item href=/categories/ title>Categories</a>
<a class=menu-item href=/projects/ title>Projects</a>
<a class=menu-item href=/about title>About</a>
<a href=javascript:void(0); class=theme-switch><i class="iconfont icon-sun"></i></a>&nbsp;</div></div><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></nav><nav class=navbar-mobile id=nav-mobile style=display:none><div class=container><div class=navbar-header><div><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-sun"></i></a>&nbsp;<a href=https://younesbelkada.github.io/>Younes Belkada</a></div><div class=menu-toggle><span></span><span></span><span></span></div></div><div class=menu id=mobile-menu><a class=menu-item href=/posts/ title>Blog</a>
<a class=menu-item href=/notes/ title>Notes</a>
<a class=menu-item href=/categories/ title>Categories</a>
<a class=menu-item href=/projects/ title>Projects</a>
<a class=menu-item href=/about title>About</a></div></div><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></nav><main class=main><div class=container><article class=post-warp><header class=post-header><h1 class=post-title>Week 1 - RL Course</h1><div class=post-meta>Written by <a href=https://younesbelkada.github.io/ rel=author>Younes Belkada</a> with â™¥
<span class=post-time>on <time datetime=2021-10-05>5 October 2021</time></span>
in
<i class="iconfont icon-folder"></i>
<span class=post-category><a href=https://younesbelkada.github.io/categories/course-notes/>Course notes</a></span>
<i class="iconfont icon-timer"></i>
5 min</div></header><div class=post-content><p>Here is the first personal notes from the <em>Reinforcement Learning</em> course that I have decided to take at the <em>MVA</em> Master&rsquo;s program. The course is quite theoretical and this is challenging for me since I have (almost) zero knowledge on Reinforcement Learning. I have decided to try to explain what I have understood in each lecture in order to assimilate the content of each session.</p><h1 id=what-is-reinforcement-learning>What is Reinforcement Learning?</h1><p>RL is a family of Machine Learning techniques in order to <strong>solve a task</strong> that is based on the <strong>current</strong> and <strong>previous states</strong> of an agent interacting with an environement. In the most practical cases, the agent learns by interacting under uncertainity (i.e. the environement is <strong>unknown</strong>).
In practice, RL can solve several problems related to portfolio management, autonomous driving, game theory and so on.</p><h3 id=probelm-statement-in-rl>Probelm statement in RL</h3><p>In a RL configuration the time needs to be discrete, i.e $t = 1, 2, 3$ thus not continuous. At each time stepm the agent:</p><ul><li>Selects and action based on the current state or on all the states</li><li>Gets a reward</li><li>moves to a new state</li></ul><p><figure><img src=/images/ring.svg data-sizes=auto data-src=/images/notes/rl1.png alt="Source of the image: https://www.researchgate.net/figure/Generalized-Reinforcement-Learning-scheme-15_fig3_329510901" class=lazyload><figcaption class=image-caption>Source of the image: https://www.researchgate.net/figure/Generalized-Reinforcement-Learning-scheme-15_fig3_329510901</figcaption></figure></p><h3 id=markov-chains-and-rl>Markov Chains and RL</h3><p>A Markov chain is a dynamic system that satifies the <strong>Markov Property</strong>:</p><p>$
\begin{equation}
\mathbb{P}(s_{t+1} | s_{t}, s_{t-1},&mldr;,s_{0}) = \mathbb{P}(s_{t+1} | s_{t})
\end{equation}
$</p><p>Also, the transition between each state is determined by a probability called <strong>transition probability</strong>. In the case of RL, we also consider the space of actions $A$, the transition probability formula becomes:</p><p>$
\begin{equation}
p(s'|s, a) = \mathbb{P}(s_{t+1} | s_{t}=s, a_{t} = a)
\end{equation}
$</p><h3 id=granularity-of-the-time-step>Granularity of the time step</h3><p>How granular needs to be the time step? The granularity will depend on each problem and needs to be <strong>fine-tuned</strong>. For e.g., for an Atari game we may need 4 frames between each step in order to notice the direction of the ball to take the optimal decision</p><h3 id=assumptions-under-a-markov-decision-process>Assumptions under a Markov Decision Process</h3><ul><li>Each transition needs to be characterized by a reward that can be <strong>deterministic</strong> or <strong>stochastic</strong>.</li><li>The dynamics and reward <strong>do not</strong> change over time</li></ul><h1 id=policies-and-decision-rules>Policies and Decision rules</h1><p>In order to solve a MDP and find the optimal solution, the final goal for the agents is to learn a <strong>Policy</strong> that is defined as a <strong>set of decision rule</strong>. The latest, is a function $d: S \mapsto A$ that takes a state $s \in S$ as an input, and outputs and action $a \in A$.
The decision rule can be:</p><ul><li>Deterministic</li><li>Stochastic</li><li>History-dependent</li><li>Markov?</li></ul><p>An agent behaving under the policy $\pi$ selects at round $t$ the action:
$a_{t} \backsim d_{t}(s_{t})$</p><h1 id=optimiality-principles>Optimiality Principles</h1><p>A metric to &lsquo;evaluate&rsquo; a policy is to compute the <strong>State Value Function</strong> $V^{\pi}$.</p><h3 id=if-the-time-horizon-is-finite>If the time horizon is finite</h3><p>$V^{\pi}(t, s) = \mathbb{E}[ \sum_{\tau = t}^{T-1} r(s_{\tau}, d_{\tau} (s_{\tau})) + R(s_{T}) | s_{t} = s; \pi = (d_1,&mldr;,d_{T})]$</p><h3 id=if-the-time-horizon-is-infinite>If the time horizon is infinite</h3><p>This is used in practice when there is an uncertainity about the deadline.</p><p>$V^{\pi}(s) = \mathbb{E}[ \sum_{t=0}^{\infty} \gamma^t r(s_{t}, d_{t} (s_{t})) | s_{0} = s; \pi = (d_1,&mldr;,d_{T})]$ With $\gamma \in [0,1)$</p><p>Basically the coefficient $\gamma$ indicates around which time step we should get most of the rewards. If $\gamma$ is small, then the STF will focus on <em>short-term rewards</em>, otherwise the function will flag <em>long-term rewards</em>.</p><h3 id=optimal-value-function>Optimal Value Function</h3><p>The solution to an MDP is an optimal policy satisfying</p><p>$\pi^{*} \in argmax V^{\pi}_{\pi \in \Pi}$</p><p>It is not an equality since we may have several optimal solutions for the problem. The corresponding value function is the optimal value function $ V^{*} = V^{\pi^{*}} $</p><h1 id=bellman-operator-and-bellman-equation>Bellman operator and Bellman equation</h1><h3 id=notations>Notations</h3><p>An example is worth thousands words. In order to define the variables that we are going to use, let&rsquo;s define a simple MDP which is the one below:</p><p><figure><img src=/images/ring.svg data-sizes=auto data-src=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png alt="Simple MDP" class=lazyload><figcaption class=image-caption>Simple MDP</figcaption></figure></p><p>The decision rule $\pi$ is a set of size $N$, with $N$ corresponding to the number of states. Each value of $\pi_i$ corresponds to the action that we take at the state $s_i$.</p><p>$
\begin{array}{c}\pi\ =\ \left\{\textcolor{#E94D40}{\pi_0},\ &mldr;\ ,\ \pi_N\right\}\\\<br>\textcolor{#E94D40}{\pi_{0\ }}\ \in\ A\ \end{array}
$</p><p>The transition probabilites matrix can be easily written. Each row $i$ corresponds to the probabilities of moving from a state $i$ to all possible states $j \in {1,..,n}$ <strong>given a decision rule $\pi$</strong>. Thus here if $\pi$ is stationnary and $\pi = \{\textcolor{#E94D40}{a_0},\ \textcolor{#E94D40}{a_0} ,\ \textcolor{#3697DC}{a_1}\}$,</p><p>$
\begin{equation*}
P =
\begin{pmatrix}
0 & 0.1 & 0.9 \\\<br>1 & 0 & 0 \\\<br>0 & 0 & 1
\end{pmatrix}
\end{equation*}
$</p><p>The reward vector $r^\pi$ is the vector containing the <em><strong>expected</strong></em> rewards at each state, given a policy $\pi$. In our case, using the same policy, we obtain</p><p>$
\begin{align}
r^\pi & = &
\begin{pmatrix}
0 \\\<br>\mathbb{E}(Be(1/3)) \\\<br>0
\end{pmatrix}
& = &
\begin{pmatrix}
0 \\\<br>1/3 \\\<br>0
\end{pmatrix}
\end{align}
$</p><p>If $\pi$ was $\{\textcolor{#E94D40}{a_0},\ \textcolor{#E94D40}{a_0} ,\ \textcolor{#E94D40}{a_0}\}$ we would obtain:</p><p>$
\begin{align}
r^\pi & = &
\begin{pmatrix}
0 \\\<br>\mathbb{E}(Be(1/3)) \\\<br>\mathbb{E}(Be(2/3))
\end{pmatrix}
& = &
\begin{pmatrix}
0 \\\<br>1/3 \\\<br>2/3
\end{pmatrix}
\end{align}
$</p><h3 id=bellman-equation>Bellman equation</h3><p>Given a state $s$ and, policy $\pi$ and a set of transisition probability, we can analytically compute the value of the Value Function in an exact way using the Bellman Equation.</p><p>$
\begin{equation}
V^{\pi}(s)=r(s, \pi(s))+\gamma \sum_{y} p(y \mid s, \pi(s)) V^{\pi}(y)
\end{equation}
$</p><p>The equation below can be written also in a matrix form:</p><p>$
\begin{aligned}
V^{\pi} &=r^{\pi}+\gamma P^{\pi} V^{\pi} \<br>\Longrightarrow & V^{\pi} &=\left(I-\gamma P^{\pi}\right)^{-1} r^{\pi}
\end{aligned}
$</p><p>which can be easily computed if <em><strong>all the variable were know beforehand,</strong></em> which in practice not really the case since the environement with which the agent interacts is unknown.</p><h3 id=bellman-operator>Bellman operator</h3><p>For any $W \in \mathbb{R}^n$, the Bellman operator is a function $\mathcal{T}^{\pi} : \mathbb{R}^n \mapsto \mathbb{R}^n$ such that:</p><p>$
\mathcal{T}^{\pi} W(s)=r(s, \pi(s))+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, \pi(s)\right) W\left(s^{\prime}\right)
$</p><p>This operator encapsulates the fundamental operation in order to compute a Value Function, given a MDP</p><p>When the policy is <em><strong>unknown</strong></em> we use the following formula to compute each component of $W$:
$
\mathcal{T} W(s)=\max _{a \in A}\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) W(s)\right]
$</p></div><div class=post-copyright><p class=copyright-item><span>Author:</span>
<span>younesbelkada</span></p><p class=copyright-item><span>Words:</span>
<span>1003</span></p><p class=copyright-item><span>Share:</span>
<span><a href="//twitter.com/share?url=https%3a%2f%2fyounesbelkada.github.io%2fnotes%2frl1%2f&text=Week%201%20-%20RL%20Course&via=" target=_blank title="Share on Twitter"><i class="iconfont icon-twitter"></i></a>
<a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fyounesbelkada.github.io%2fnotes%2frl1%2f" target=_blank title="Share on Facebook"><i class="iconfont icon-facebook"></i></a>
<a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fyounesbelkada.github.io%2fnotes%2frl1%2f&title=Week%201%20-%20RL%20Course" target=_blank title="Share on LinkedIn"><i class="iconfont icon-linkedin"></i></a></span></p><p class=copyright-item>Released under <a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></div><div class=post-tags><section><a href=javascript:window.history.back();>Back</a></span> Â·
<span><a href=https://younesbelkada.github.io/>Home</a></span></section></div><div class=post-nav><a href=https://younesbelkada.github.io/notes/rl2/ class=next rel=next title="Week 2 - RL Course">Week 2 - RL Course&nbsp;<i class="iconfont icon-xiaojiantou"></i></a></div><div class=post-comment></div></article></div></main><footer class=footer><div class=copyright>&copy;
<span itemprop=copyrightYear>2021 - 2021</span>
<span class=author itemprop=copyrightHolder><a href=https://younesbelkada.github.io/>younesbelkada</a></span></div><a title="Real Time Web Analytics" href=http://clicky.com/101337009><img alt=Clicky src=//static.getclicky.com/media/links/badge.gif border=0></a>
<script async src=//static.getclicky.com/101337009.js></script><noscript><p><img alt=Clicky width=1 height=1 src=//in.getclicky.com/101337009ns.gif></p></noscript></footer><link crossorigin=anonymous integrity=sha384-yziQACfvCVwLqVFLqkWBYRO3XeA4EqzfXKGwaWnenYn5XzqfJFlFdKEmvutIQdKb href=https://lib.baomitu.com/lightgallery/1.10.0/css/lightgallery.min.css rel=stylesheet><script src=/js/vendor_gallery.min.js async></script></div></body></html>