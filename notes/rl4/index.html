<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Week 4 - RL Course - Younes Belkada's blog</title><meta name=description content="Younes Belkada's blog"><meta property="og:title" content="Week 4 - RL Course"><meta property="og:description" content="Model-free Reinforcement Learning We previously saw various cases of how to solve an MDP when the MDP is given (i.e. known transition probabilities and reward function). Often the problems are not that simple, as we call them model-free. The agent will only have access to a black box interacting with him.
We saw in this case that we can solve the MDP using several techniques such as Monte Carlo learning, that looks at several trajectories and estimates the empirical Value Function, or TD(0) learning that bootstraps the reward function of the next states and computes the difference between the expect reward at the next state and the observed reward at the next state."><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/notes/rl4/"><meta property="og:image" content="http://example.org/logo.png"><meta property="article:section" content="notes"><meta property="article:published_time" content="2021-10-26T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-26T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://example.org/logo.png"><meta name=twitter:title content="Week 4 - RL Course"><meta name=twitter:description content="Model-free Reinforcement Learning We previously saw various cases of how to solve an MDP when the MDP is given (i.e. known transition probabilities and reward function). Often the problems are not that simple, as we call them model-free. The agent will only have access to a black box interacting with him.
We saw in this case that we can solve the MDP using several techniques such as Monte Carlo learning, that looks at several trajectories and estimates the empirical Value Function, or TD(0) learning that bootstraps the reward function of the next states and computes the difference between the expect reward at the next state and the observed reward at the next state."><meta name=application-name content="Younes Belkada"><meta name=apple-mobile-web-app-title content="Younes Belkada"><meta name=theme-color content="#ffc40d"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=http://example.org/notes/rl4/><link rel=prev href=http://example.org/notes/rl3/><link rel=stylesheet href=/css/page.min.css><link rel=stylesheet href=/css/home.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Week 4 - RL Course","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/example.org\/notes\/rl4\/"},"genre":"notes","keywords":"Reinforcement Learning, Maths, Markov Chains","wordcount":786,"url":"http:\/\/example.org\/notes\/rl4\/","datePublished":"2021-10-26T00:00:00+00:00","dateModified":"2021-10-26T00:00:00+00:00","publisher":{"@type":"Organization","name":"Younes Belkada"},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body data-header-desktop data-header-mobile><script>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/>About </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/notes/>Notes </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title>About</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/notes/ title>Notes</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><div class=menu-item><a href=javascript:void(0); class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><main class=main><div class=container><div class=toc id=toc-auto style=top:8rem><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single special" data-toc=enable><h2 class="single-title animated fadeInDown faster">Week 4 - RL Course</h2><div class=single-card><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#model-free-reinforcement-learning>Model-free Reinforcement Learning</a><ul><li><a href=#model-free-prediction--model-free-policy-evaluation>Model-free Prediction / Model-free Policy evaluation</a><ul><li><a href=#monte-carlo-learning>Monte Carlo learning</a><ul><li><a href=#first-visit-monte-carlo>First visit Monte Carlo</a></li><li><a href=#every-visit-monte-carlo>Every visit Monte Carlo</a></li><li><a href=#incremental-monte-carlo>Incremental Monte Carlo</a></li></ul></li><li><a href=#temporal-difference-learning>Temporal Difference Learning</a><ul><li><a href=#td0-algorithm>TD(0) algorithm</a></li><li><a href=#tdlambda-algorithm>TD($\lambda$) algorithm</a></li><li><a href=#bias--variance-trade-off>Bias / Variance trade-off</a></li></ul></li><li><a href=#td-vs-mc>TD vs MC</a></li></ul></li><li><a href=#model-free-control--model-free-policy-learning>Model-free Control / Model-free Policy learning</a></li><li><a href=#sarsa>SARSA</a></li><li><a href=#q-learning>Q-learning</a></li></ul></li></ul></nav></div></div><div class=content id=content><h2 id=model-free-reinforcement-learning>Model-free Reinforcement Learning</h2><p>We previously saw various cases of how to solve an MDP when the MDP is given (i.e. known transition probabilities and reward function). Often the problems are not that simple, as we call them <em>model-free</em>. The agent will only have access to a black box interacting with him.</p><p>We saw in this case that we can solve the MDP using several techniques such as Monte Carlo learning, that looks at several trajectories and estimates the empirical Value Function, or TD(0) learning that <em>bootstraps</em> the reward function of the next states and computes the difference between the expect reward at the next state and the observed reward at the next state.</p><div class="details admonition Abstract open"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw"></i>Control vs Prediction<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content><ul><li><em>Prediction</em> stands for evaluating the <strong>given</strong> policy for a MDP</li><li><em>Control</em> stands for learning the optimal policy given the MDP.</li></ul></div></div></div><h3 id=model-free-prediction--model-free-policy-evaluation>Model-free Prediction / Model-free Policy evaluation</h3><h4 id=monte-carlo-learning>Monte Carlo learning</h4><p>The idea behind Monte Carlo learning si extremely simple. After getting $k$ trajectories, we estimate the Value Function by the expectation of the total discounted reward of each state.
<em>How do we update the Value Function for any state $s$?</em></p><h5 id=first-visit-monte-carlo>First visit Monte Carlo</h5><p>For each episode:</p><ul><li>In the first time step $t$ that state $s$ is visited in an episode:</li></ul><ul><li>Increment counter $N(s) \mapsto N(s) + 1$</li><li>Increment total return $S(s) \mapsto S(s) + G_t$</li><li>Value is estimated by mean return $V(s) = S(s) / N(s)$</li><li>By law of large numbers, $V(s) \mapsto V^{\pi} (s)$ as $N(s) \mapsto \infty$</li></ul><p>We evaluate the state $s$ by taking into account the reward only for the <strong>first visit</strong> at each episode.</p><h5 id=every-visit-monte-carlo>Every visit Monte Carlo</h5><p>For each episode:</p><ul><li><strong>Every</strong> time step $t$ that state $s$ is visited in an episode:</li></ul><ul><li>Increment counter $N(s) \mapsto N(s) + 1$</li><li>Increment total return $S(s) \mapsto S(s) + G_t$</li><li>Value is estimated by mean return $V(s) = S(s) / N(s)$</li><li>By law of large numbers, $V(s) \mapsto V^{\pi} (s)$ as $N(s) \mapsto \infty$</li></ul><h5 id=incremental-monte-carlo>Incremental Monte Carlo</h5><p>The mean of a sequence can be computed incrementally
$$
\begin{equation*}
\mu_k = \frac{1}{k} \sum_{i=0}^k x_i
\end{equation*}
$$
$$
\begin{equation*}
\mu_k = \frac{1}{k} (x_k + \sum_{i=0}^{k-1} x_i)
\end{equation*}
$$
$$
\begin{equation*}
\mu_k = \frac{1}{k} (x_k + (k-1)\mu_{k-1})
\end{equation*}
$$
$$
\begin{equation*}
\mu_k = \mu_{k-1} + \frac{1}{k} (x_k -\mu_{k-1})
\end{equation*}
$$</p><p>For each episode:</p><ul><li>For each state $S_t$ with return $G_t$:</li></ul><ul><li>$N(S_t) \mapsto N(S_t) + 1$</li><li>$V(S_t) \mapsto V(S_t) + \frac{1}{N(S_t)} (G_t - V(S_t)) $</li></ul><h4 id=temporal-difference-learning>Temporal Difference Learning</h4><ul><li>TD methods learn directly from episodes of experience</li><li>TD is model free</li><li>TD learns from incomplete episodes by bootstrapping</li><li>TD updates a guess towards a guess</li></ul><h5 id=td0-algorithm>TD(0) algorithm</h5><p>The idea is to update the value $V(S_t)$ toward the <strong>estimated return</strong> $R_{t+1} + \gamma V(S_{t+1})$</p><ul><li>V$(S_t) \mapsto V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))$</li></ul><p>We want the estimated Value Function to update in the direction of the estimated immediate reward! Useful when the reward on the current state does not really depend on the result after doing a whole trajectory.</p><h5 id=tdlambda-algorithm>TD($\lambda$) algorithm</h5><p>Let TD target look $n$ steps into the future! That is the main idea behind TD($\lambda$) algorithm. let us define the n-step return as the following:</p><ul><li>$n = 1$ $G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1})$ (TD)</li><li>$n = 2$ $G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$</li><li>$n = \infty$ $G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + &mldr; + \gamma^{T-1} R_T$ (MC)</li></ul><p>Which $n$ is the best?</p><h5 id=bias--variance-trade-off>Bias / Variance trade-off</h5><ul><li>TD target $R_{t+1} + \gamma V(S_{t+1})$ is biased estimate of $V^{\pi} (S_t)$</li><li>TD target is much lower variance than the return (MC estimate).</li></ul><h4 id=td-vs-mc>TD vs MC</h4><ul><li><p>TD can learn before knowing the final outcome</p><ul><li>TD can learn online after every step</li><li>MC must wait until end of episode before return in known</li></ul></li><li><p>TD can learn without the final outcome</p><ul><li>TD can learn from incomplete sequences</li><li>MC can only learn from complete sequences</li><li>TD works in continuing (non-terminating) environments</li><li>MC only works for episodic (terminating) environments</li></ul></li><li><p>MC has high variance, zero bias</p><ul><li>Good convergence properties</li><li>Not very sensitive to initial value</li><li>Vary simple to understand and use</li></ul></li><li><p>TD has low variance, zero bias</p><ul><li>Usually more efficient than MC</li><li>TD(0) converges to $V^{\pi} (s)$</li><li>More sensitive to initial value</li></ul></li></ul><h3 id=model-free-control--model-free-policy-learning>Model-free Control / Model-free Policy learning</h3><h3 id=sarsa>SARSA</h3><p><strong>How do we learn the best policy??</strong> A family of methods that can be used is called <em>Actor-Critic methods</em>.</p><figure><a class=lightgallery href=/images/notes/rl-actor-critic.png title=/images/notes/rl-actor-critic.png data-thumbnail=/images/notes/rl-actor-critic.png data-sub-html="<h2>Actor Critic methods</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/rl-actor-critic.png data-srcset="/images/notes/rl-actor-critic.png, /images/notes/rl-actor-critic.png 1.5x, /images/notes/rl-actor-critic.png 2x" data-sizes=auto alt=/images/notes/rl-actor-critic.png height=500 width=500></a><figcaption class=image-caption>Actor Critic methods</figcaption></figure><p>The idea here is that the two components act between each other, at each iteration $k$ the Critic <em>evaluates</em> the policy $\pi^k$ and the Actor <em>improves</em> the policy by acting greedily.</p><p>The Critic will use the TD learning to update the estimated Value function (or Q-function)</p><p>$$
\delta_{t}=r_{t}+\gamma \widehat{Q}\left(s_{t+1}, a_{t+1}\right)-\widehat{Q}\left(s_{t}, a_{t}\right)
$$</p><p>$$
\widehat{Q}\left(s_{t}, a_{t}\right)=\widehat{Q}\left(s_{t}, a_{t}\right)+\alpha\left(s_{t}, a_{t}\right) \delta_{t}
$$</p><h3 id=q-learning>Q-learning</h3></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-tag><span><a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
</span><span><a href=/tags/maths/>Maths</a>
</span><span><a href=/tags/markov-chains/>Markov Chains</a></span></div><div class=post-info-line><div class=post-info-mod><span>Updated on 2021-10-26</span></div><div class=post-info-mod></div></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=http://example.org/notes/rl4/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="Share on WhatsApp" data-sharer=whatsapp data-url=http://example.org/notes/rl4/ data-title="Week 4 - RL Course" data-web><i class="fab fa-whatsapp fa-fw"></i></a></span></div></div><div class=post-nav><a href=/notes/rl3/ class=prev rel=prev title="Week 3 - RL Course"><i class="fas fa-angle-left fa-fw"></i>Previous Post</a></div></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.85.0">Hugo</a> | Theme - <a href=https://github.com/khusika/FeelIt target=_blank rel="noopener noreffer" title="FeelIt 1.0.1"><i class="fas fa-hand-holding-heart fa-fw"></i> FeelIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/>Younes Belkada</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-chevron-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment-alt fa-fw"></i></a></div><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><script src=/lib/lazysizes/lazysizes.min.js></script><script src=/lib/lightgallery/lightgallery.min.js></script><script src=/lib/lightgallery/lg-thumbnail.min.js></script><script src=/lib/lightgallery/lg-zoom.min.js></script><script src=/lib/clipboard/clipboard.min.js></script><script src=/lib/sharer/sharer.min.js></script><script src=/lib/katex/katex.min.js></script><script src=/lib/katex/auto-render.min.js></script><script src=/lib/katex/copy-tex.min.js></script><script src=/lib/katex/mhchem.min.js></script><script>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script src=/js/theme.min.js></script></body></html>