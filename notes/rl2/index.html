<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Week 2 - RL Course - Younes Belkada's blog</title><meta name=description content="Younes Belkada's blog"><meta property="og:title" content="Week 2 - RL Course"><meta property="og:description" content="In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.
Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem."><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/notes/rl2/"><meta property="og:image" content="http://example.org/logo.png"><meta property="article:section" content="notes"><meta property="article:published_time" content="2021-10-12T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-12T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://example.org/logo.png"><meta name=twitter:title content="Week 2 - RL Course"><meta name=twitter:description content="In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.
Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem."><meta name=application-name content="Younes Belkada"><meta name=apple-mobile-web-app-title content="Younes Belkada"><meta name=theme-color content="#ffc40d"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=http://example.org/notes/rl2/><link rel=prev href=http://example.org/notes/rl1/><link rel=stylesheet href=/css/page.min.css><link rel=stylesheet href=/css/home.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Week 2 - RL Course","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/example.org\/notes\/rl2\/"},"genre":"notes","keywords":"Reinforcement Learning, Maths, Markov Chains","wordcount":593,"url":"http:\/\/example.org\/notes\/rl2\/","datePublished":"2021-10-12T00:00:00+00:00","dateModified":"2021-10-12T00:00:00+00:00","publisher":{"@type":"Organization","name":"Younes Belkada"},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body data-header-desktop data-header-mobile><script>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/>About </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/notes/>Notes </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title>About</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/notes/ title>Notes</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><div class=menu-item><a href=javascript:void(0); class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><main class=main><div class=container><div class=toc id=toc-auto style=top:8rem><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single special" data-toc=enable><h2 class="single-title animated fadeInDown faster">Week 2 - RL Course</h2><div class=single-card><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#intuition>Intuition</a></li><li><a href=#theorem>Theorem</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.</p><p>Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem. Few techniques exists in order to achieve this, let&rsquo;s start with the <strong>Value Iteration</strong></p><h1 id=value-iteration>Value Iteration</h1><figure><a class=lightgallery href=/images/notes/rl_value_iteration.gif title=https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2 data-thumbnail=/images/notes/rl_value_iteration.gif data-sub-html="<h2>Value Iteration</h2><p>https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/rl_value_iteration.gif data-srcset="/images/notes/rl_value_iteration.gif, /images/notes/rl_value_iteration.gif 1.5x, /images/notes/rl_value_iteration.gif 2x" data-sizes=auto alt=/images/notes/rl_value_iteration.gif height=500 width=500></a><figcaption class=image-caption>Value Iteration</figcaption></figure><h3 id=intuition>Intuition</h3><p>We have few tools in our toolbox. First of all, <em>how do we evaluate a policy?</em> We can simply compute the <strong>Value Function</strong> which corresponds to a kind of &ldquo;summary&rdquo; of the rewards at each state, given a policy. If we consider again the simple MDP considered last week, if the policy is $\pi = \{\textcolor{#E94D40}{a_0},\ \textcolor{#E94D40}{a_0} ,\ \textcolor{#3697DC}{a_1}\}$</p><figure><a class=lightgallery href=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png title=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-thumbnail=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-sub-html="<h2>Simple MDP</h2><p>https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png</p>"><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-srcset="https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png, https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png 1.5x, https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png 2x" data-sizes=auto alt=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png height=300 width=300></a><figcaption class=image-caption>Simple MDP</figcaption></figure><p>Analytically, The <strong>Value Function</strong> is equal to $V^\pi = [0.033, 0.363, 0]$. Which means intuitively, that in <em>expectation</em> under the <strong>given</strong> policy, moving frequently to the state $s_1$ is highly <strong>benefic</strong> in terms of reward.</p><p><em>What if the policy is unknown?</em> Then the problem would have a different formulation, our goal would be to <strong>find the best policy possible</strong> among the combination of all possible policies. You can image that this is doable if the problem is simple enough ($|A| &lt; N_A$ and $|S| &lt; N_S$, i.e reasonable number of possible actions and reasonable number of states) but extremely complex if not.</p><p>The idea of the <strong>Value Iteration</strong> algorithm is the following:</p><ul><li>Initialize $V_0$ in $R^N$</li><li>At each iteration $k = 1,2,&mldr;,K$:<ul><li>Compute $V_{k+1} = \mathcal{T}V_k$ until $|| V_{k+1} - V_k || \leq \epsilon$</li></ul></li><li>Get the <em><strong>greedy policy</strong></em> $
\pi_{K}(s) \in \arg \max _{a \in A}\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) V_{K}\left(s^{\prime}\right)\right]
$</li></ul><p>Recall that the greedy policy is $\in$ the set of $argmax$ because we may have several values that reaches the maximum.</p><p>Intuitively, the algorithm would work as the followwing</p><ul><li>Initialize with a random <strong>Value Function</strong></li><li>Compute the Bellman operator applied to this <strong>Value Function</strong> for all <strong>the possible actions</strong> and update $V_k$ at each step by considering the action that maximized the <strong>Value Function</strong> (i.e. the optimal action)</li></ul><p>Let&rsquo;s consider the following problem, where the initial state is $S_0$ and the final state $S_3$. The set of actions here is $A = \{a_0, a_1, a_2, a_3\}$ which corresponds to the action of moving to a corresponding state. We can imagine that it is a <em>maze</em> where the agent has to start from the state 0 and tries to learn how to escape from there.</p><figure><a class=lightgallery href=/images/notes/diag_rl1_value_iteration.png title="Original Content" data-thumbnail=/images/notes/diag_rl1_value_iteration.png data-sub-html="<h2>Another simple MDP for our example</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/diag_rl1_value_iteration.png data-srcset="/images/notes/diag_rl1_value_iteration.png, /images/notes/diag_rl1_value_iteration.png 1.5x, /images/notes/diag_rl1_value_iteration.png 2x" data-sizes=auto alt=/images/notes/diag_rl1_value_iteration.png height=300 width=300></a><figcaption class=image-caption>Another simple MDP for our example</figcaption></figure><p>Here the process is still a MDP because it respects the assumptions under <strong>Markov Decision Processes</strong>. We ignore in our example the transition probabilities for simplicity and apply the algorithm for one step.</p><figure><a class=lightgallery href=/images/notes/rl_diag_1.gif title="Original Content" data-thumbnail=/images/notes/rl_diag_1.gif data-sub-html="<h2>Value Iteration Algorithm for one step - Original content</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/rl_diag_1.gif data-srcset="/images/notes/rl_diag_1.gif, /images/notes/rl_diag_1.gif 1.5x, /images/notes/rl_diag_1.gif 2x" data-sizes=auto alt=/images/notes/rl_diag_1.gif></a><figcaption class=image-caption>Value Iteration Algorithm for one step - Original content</figcaption></figure><p>After one step, the optimal policy is the following:</p><p>$$ \begin{array}{l}V_1\ =\ \left[1,\ 10,\ 10,\ 10\right]\\\ \pi^* =\ \left\{\left[a_1,\ a_3,\ a_3,\ a_3\right],\ \left[a_2,\ a_3,\ a_3,\ a_3\right]\right\}\end{array} $$</p><p>Which absolutely makes sense, because in this MDP, to reach an optimal reward value you should avoid moving to the state $s_0$ and move to $s_3$ whenever you can.</p><h3 id=theorem>Theorem</h3><p>Thankfully, we have a guarantee that the algorithm will give us a solution in some fixed number of steps.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-tag><span><a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
</span><span><a href=/tags/maths/>Maths</a>
</span><span><a href=/tags/markov-chains/>Markov Chains</a></span></div><div class=post-info-line><div class=post-info-mod><span>Updated on 2021-10-12</span></div><div class=post-info-mod></div></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=http://example.org/notes/rl2/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="Share on WhatsApp" data-sharer=whatsapp data-url=http://example.org/notes/rl2/ data-title="Week 2 - RL Course" data-web><i class="fab fa-whatsapp fa-fw"></i></a></span></div></div><div class=post-nav><a href=/notes/rl1/ class=prev rel=prev title="Week 1 - RL Course"><i class="fas fa-angle-left fa-fw"></i>Previous Post</a></div></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.85.0">Hugo</a> | Theme - <a href=https://github.com/khusika/FeelIt target=_blank rel="noopener noreffer" title="FeelIt 1.0.1"><i class="fas fa-hand-holding-heart fa-fw"></i> FeelIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/>Younes Belkada</a></span></div><a title="Real Time Web Analytics" href=http://clicky.com/101337009><img alt=Clicky src=//static.getclicky.com/media/links/badge.gif border=0></a>
<script async src=//static.getclicky.com/101337009.js></script><noscript><p><img alt=Clicky width=1 height=1 src=//in.getclicky.com/101337009ns.gif></p></noscript></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-chevron-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment-alt fa-fw"></i></a></div><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><script src=/lib/lazysizes/lazysizes.min.js></script><script src=/lib/lightgallery/lightgallery.min.js></script><script src=/lib/lightgallery/lg-thumbnail.min.js></script><script src=/lib/lightgallery/lg-zoom.min.js></script><script src=/lib/clipboard/clipboard.min.js></script><script src=/lib/sharer/sharer.min.js></script><script src=/lib/katex/katex.min.js></script><script src=/lib/katex/auto-render.min.js></script><script src=/lib/katex/copy-tex.min.js></script><script src=/lib/katex/mhchem.min.js></script><script>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script src=/js/theme.min.js></script></body></html>