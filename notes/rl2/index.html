<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Week 2 - RL Course - Younes Belkada's blog</title><meta name=description content="Younes Belkada's blog"><meta property="og:title" content="Week 2 - RL Course"><meta property="og:description" content="In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.
Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem."><meta property="og:type" content="article"><meta property="og:url" content="http://example.org/notes/rl2/"><meta property="og:image" content="http://example.org/logo.png"><meta property="article:section" content="notes"><meta property="article:published_time" content="2021-10-12T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-12T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://example.org/logo.png"><meta name=twitter:title content="Week 2 - RL Course"><meta name=twitter:description content="In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.
Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem."><meta name=application-name content="Younes Belkada"><meta name=apple-mobile-web-app-title content="Younes Belkada"><meta name=theme-color content="#ffc40d"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=http://example.org/notes/rl2/><link rel=prev href=http://example.org/notes/rl1/><link rel=next href=http://example.org/notes/rl3/><link rel=stylesheet href=/css/page.min.css><link rel=stylesheet href=/css/home.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Week 2 - RL Course","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/example.org\/notes\/rl2\/"},"genre":"notes","keywords":"Reinforcement Learning, Maths, Markov Chains","wordcount":999,"url":"http:\/\/example.org\/notes\/rl2\/","datePublished":"2021-10-12T00:00:00+00:00","dateModified":"2021-10-12T00:00:00+00:00","publisher":{"@type":"Organization","name":"Younes Belkada"},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body data-header-desktop data-header-mobile><script>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/>About </a><a class=menu-item href=/projects/>Projects </a><a class=menu-item href=/notes/>Notes </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Younes Belkada's blog">Younes Belkada's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title>About</a><a class=menu-item href=/projects/ title>Projects</a><a class=menu-item href=/notes/ title>Notes</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><div class=menu-item><a href=javascript:void(0); class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><main class=main><div class=container><div class=toc id=toc-auto style=top:8rem><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single special" data-toc=enable><h2 class="single-title animated fadeInDown faster">Week 2 - RL Course</h2><div class=single-card><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#value-iteration>Value Iteration</a><ul><li><a href=#intuition>Intuition</a></li></ul></li><li><a href=#policy-iteration>Policy Iteration</a></li><li><a href=#monte-carlo-for-policy-evaluation>Monte Carlo for Policy Evaluation</a><ul><li><a href=#every-visit-monte-carlo>Every Visit Monte Carlo</a></li></ul></li><li><a href=#temporal-difference-learning>Temporal Difference Learning</a><ul><li><a href=#td0-estimation>TD(0) Estimation</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.</p><p>Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem. Few techniques exists in order to achieve this, let&rsquo;s start with the <strong>Value Iteration</strong></p><h2 id=value-iteration>Value Iteration</h2><figure><a class=lightgallery href=/images/notes/rl_value_iteration.gif title=https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2 data-thumbnail=/images/notes/rl_value_iteration.gif data-sub-html="<h2>Value Iteration</h2><p>https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/rl_value_iteration.gif data-srcset="/images/notes/rl_value_iteration.gif, /images/notes/rl_value_iteration.gif 1.5x, /images/notes/rl_value_iteration.gif 2x" data-sizes=auto alt=/images/notes/rl_value_iteration.gif height=500 width=500></a><figcaption class=image-caption>Value Iteration</figcaption></figure><h3 id=intuition>Intuition</h3><p>We have few tools in our toolbox. First of all, <em>how do we evaluate a policy?</em> We can simply compute the <strong>Value Function</strong> which corresponds to a kind of &ldquo;summary&rdquo; of the rewards at each state, given a policy. If we consider again the simple MDP considered last week, if the policy is $\pi = \{\textcolor{#E94D40}{a_0},\ \textcolor{#E94D40}{a_0} ,\ \textcolor{#3697DC}{a_1}\}$</p><figure><a class=lightgallery href=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png title=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-thumbnail=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-sub-html="<h2>Simple MDP</h2><p>https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png</p>"><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png data-srcset="https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png, https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png 1.5x, https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png 2x" data-sizes=auto alt=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png height=300 width=300></a><figcaption class=image-caption>Simple MDP</figcaption></figure><p>Analytically, The <strong>Value Function</strong> is equal to $V^\pi = [0.033, 0.363, 0]$. Which means intuitively, that in <em>expectation</em> under the <strong>given</strong> policy, moving frequently to the state $s_1$ is highly <strong>benefic</strong> in terms of reward.</p><p><em>What if the policy is unknown?</em> Then the problem would have a different formulation, our goal would be to <strong>find the best policy possible</strong> among the combination of all possible policies. You can image that this is doable if the problem is simple enough ($|A| &lt; N_A$ and $|S| &lt; N_S$, i.e reasonable number of possible actions and reasonable number of states) but extremely complex if not.</p><p>The idea of the <strong>Value Iteration</strong> algorithm is the following:</p><ul><li>Initialize $V_0$ in $R^N$</li><li>At each iteration $k = 1,2,&mldr;,K$:<ul><li>Compute $V_{k+1} = \mathcal{T}V_k$ until $|| V_{k+1} - V_k || \leq \epsilon$</li></ul></li><li>Get the <em><strong>greedy policy</strong></em> $
\pi_{K}(s) \in \arg \max _{a \in A}\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) V_{K}\left(s^{\prime}\right)\right]
$</li></ul><p>Recall that the greedy policy is $\in$ the set of $argmax$ because we may have several values that reaches the maximum.</p><p>Intuitively, the algorithm would work as the followwing</p><ul><li>Initialize with a random <strong>Value Function</strong></li><li>Compute the Bellman operator applied to this <strong>Value Function</strong> for all <strong>the possible actions</strong> and update $V_k$ at each step by considering the action that maximized the <strong>Value Function</strong> (i.e. the optimal action)</li></ul><p>Let&rsquo;s consider the following problem, where the initial state is $S_0$ and the final state $S_3$. The set of actions here is $A = \{a_0, a_1, a_2, a_3\}$ which corresponds to the action of moving to a corresponding state. We can imagine that it is a <em>maze</em> where the agent has to start from the state 0 and tries to learn how to escape from there.</p><figure><a class=lightgallery href=/images/notes/diag_rl1_value_iteration.png title="Original Content" data-thumbnail=/images/notes/diag_rl1_value_iteration.png data-sub-html="<h2>Another simple MDP for our example</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/diag_rl1_value_iteration.png data-srcset="/images/notes/diag_rl1_value_iteration.png, /images/notes/diag_rl1_value_iteration.png 1.5x, /images/notes/diag_rl1_value_iteration.png 2x" data-sizes=auto alt=/images/notes/diag_rl1_value_iteration.png height=300 width=300></a><figcaption class=image-caption>Another simple MDP for our example</figcaption></figure><p>Here the process is still a MDP because it respects the assumptions under <strong>Markov Decision Processes</strong>. We ignore in our example the transition probabilities for simplicity and apply the algorithm for one step.</p><figure><a class=lightgallery href=/images/notes/rl_diag_1.gif title="Original Content" data-thumbnail=/images/notes/rl_diag_1.gif data-sub-html="<h2>Value Iteration Algorithm for one step - Original content</h2><p>Original Content</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/notes/rl_diag_1.gif data-srcset="/images/notes/rl_diag_1.gif, /images/notes/rl_diag_1.gif 1.5x, /images/notes/rl_diag_1.gif 2x" data-sizes=auto alt=/images/notes/rl_diag_1.gif></a><figcaption class=image-caption>Value Iteration Algorithm for one step - Original content</figcaption></figure><p>After one step, the optimal policy is the following:</p><p>$$ \begin{array}{l}V_1\ =\ \left[1,\ 10,\ 10,\ 10\right]\\\ \pi^* =\ \left\{\left[a_1,\ a_3,\ a_3,\ a_3\right],\ \left[a_2,\ a_3,\ a_3,\ a_3\right]\right\}\end{array} $$</p><p>Which absolutely makes sense, because in this MDP, to reach an optimal reward value you should avoid moving to the state $s_0$ and move to $s_3$ whenever you can.</p><h2 id=policy-iteration>Policy Iteration</h2><p>Policy iteration is another algorithm to find the optimal policy when the transition probabilities and the reward function are known, the configuration of the problem is the same as before.</p><p>Instead of starting with an arbitraty Value function, we start the algorithm with an arbitrary policy $ \pi_0 $.</p><div class="details admonition Abstract open"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw"></i>Description of the algorithm<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content><ol><li>start the algorithm with an arbitrary policy $ \pi_0 $</li><li>For each $k = 1,&mldr;,N$:<ol><li>Compute the Value Function associated with $\pi_k$ using the Bellman operator</li><li>Compute the greedy policy $$ \pi_{k+1} \in arg max [r(s, a) + \gamma \sum_{s'} p(s' | s, a) V^{\pi_k} (s')] $$</li></ol></li><li>Stop if $V^{\pi_k} = V^{\pi_{k-1}} $</li></ol></div></div></div><p>The main difference here is that for each step, the policy is supposed to be known. We can use the explicit Bellman Operator to calculate at each step the Value Function associated with the policy. Also, the policy is updated at each iteration where on the Value Iteration it is explicitly computed at the end of the algorithm.</p><p>This can be done only if the transition probabilities are known beforehand. What if we do not have access to them? The agent needs to <strong>learn by its own experience</strong>.</p><h2 id=monte-carlo-for-policy-evaluation>Monte Carlo for Policy Evaluation</h2><p><strong>How to figure out V for unknown MDP ( assume we get the policy)??</strong></p><p>Assume we have a fixed policy $\pi$. We can estimate the value function empirically, by executing (from the same initial state) $n$ trajectories and estimating the Value function using:
$$
\hat{V_n} (s_0) = \frac{1}{n} \sum^{n}_{i=1} \hat{R_i} (s_0)
$$</p><p>With $\hat{R_i} (s_0)$ corresponding to the estimated reward of the trajectory $i$. A theorem ensures that the Monte Carlo estimator converges to the Value Function.</p><p>We saw that with this approach we can only estimate the value function for $s_0$. How do we estimate it for other states?</p><h3 id=every-visit-monte-carlo>Every Visit Monte Carlo</h3><p>After the $i$th trajectory, instead of updating only $s_0$, for all $k=T_i-1$ down to $0$:
$$
\widehat{V}\left(s_{k, i}\right)=\widehat{V}\left(s_{k, i}\right)+\alpha_{i}\left(s_{k, i}\right)\left(\sum_{t=k}^{T_{i}} \gamma^{t-k} r_{t, i}-\widehat{V}\left(s_{k, i}\right)\right)
$$</p><h2 id=temporal-difference-learning>Temporal Difference Learning</h2><h3 id=td0-estimation>TD(0) Estimation</h3><p>At each state $s_t$ we observe the next state $s_{t+1}$ and the reward at the current state $r_t$. At this point, we compute the **temporal difference** as the weighted difference between the Value Function at the current state and the estimated Value Function at the next state.
$$
\delta_t = r_t + \gamma \hat{V^{\pi}} (s_{t+1}) - \hat{V^{\pi}} (s_t)
$$</p><p>The Value function at the state $s_t$ will be updated as the following:</p><p>$$
\hat{V^{\pi}}(s_t) = \hat{V^{\pi}} (s_t) + \alpha_t \delta_t
$$</p><p>This is done for each episode (i.e. each predicted trajectory)</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-tag><span><a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
</span><span><a href=/tags/maths/>Maths</a>
</span><span><a href=/tags/markov-chains/>Markov Chains</a></span></div><div class=post-info-line><div class=post-info-mod><span>Updated on 2021-10-12</span></div><div class=post-info-mod></div></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=http://example.org/notes/rl2/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="Share on WhatsApp" data-sharer=whatsapp data-url=http://example.org/notes/rl2/ data-title="Week 2 - RL Course" data-web><i class="fab fa-whatsapp fa-fw"></i></a></span></div></div><div class=post-nav><a href=/notes/rl1/ class=prev rel=prev title="Week 1 - RL Course"><i class="fas fa-angle-left fa-fw"></i>Previous Post</a>
<a href=/notes/rl3/ class=next rel=next title="Week 3 - RL Course">Next Post<i class="fas fa-angle-right fa-fw"></i></a></div></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.85.0">Hugo</a> | Theme - <a href=https://github.com/khusika/FeelIt target=_blank rel="noopener noreffer" title="FeelIt 1.0.1"><i class="fas fa-hand-holding-heart fa-fw"></i> FeelIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/>Younes Belkada</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-chevron-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment-alt fa-fw"></i></a></div><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><script src=/lib/lazysizes/lazysizes.min.js></script><script src=/lib/lightgallery/lightgallery.min.js></script><script src=/lib/lightgallery/lg-thumbnail.min.js></script><script src=/lib/lightgallery/lg-zoom.min.js></script><script src=/lib/clipboard/clipboard.min.js></script><script src=/lib/sharer/sharer.min.js></script><script src=/lib/katex/katex.min.js></script><script src=/lib/katex/auto-render.min.js></script><script src=/lib/katex/copy-tex.min.js></script><script src=/lib/katex/mhchem.min.js></script><script>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script src=/js/theme.min.js></script></body></html>