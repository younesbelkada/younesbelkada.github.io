<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><link rel=prev href=https://younesbelkada.github.io/notes/rl1/><link rel=canonical href=https://younesbelkada.github.io/notes/rl2/><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title>Week 2 - RL Course | Younes Belkada</title><meta name=title content="Week 2 - RL Course | Younes Belkada"><link rel=stylesheet href=/font/iconfont.css><link rel=stylesheet href=/css/main.min.css><meta name=twitter:card content="summary"><meta name=twitter:title content="Week 2 - RL Course"><meta name=twitter:description content="In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.
Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem."><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Week 2 - RL Course","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/younesbelkada.github.io\/notes\/rl2\/"},"image":{"@type":"ImageObject","url":"https:\/\/younesbelkada.github.io\/cover.png","width":800,"height":600},"genre":"notes","wordcount":549,"url":"https:\/\/younesbelkada.github.io\/notes\/rl2\/","datePublished":"2021-10-12T00:00:00\u002b00:00","dateModified":"2021-10-12T00:00:00\u002b00:00","publisher":{"@type":"Organization","name":"Fastbyte01","logo":{"@type":"ImageObject","url":"https:\/\/younesbelkada.github.io\/logo.png","width":127,"height":40}},"author":{"@type":"Person","name":"Younes Belkada"},"description":""}</script></head><body><div class=wrapper><nav class=navbar><div class=container><div class="navbar-header header-logo"><a href=https://younesbelkada.github.io/>Younes Belkada</a></div><div class="menu navbar-right"><a class=menu-item href=/posts/ title>Blog</a>
<a class=menu-item href=/notes/ title>Notes</a>
<a class=menu-item href=/categories/ title>Categories</a>
<a class=menu-item href=/projects/ title>Projects</a>
<a class=menu-item href=/about title>About</a>
<a href=javascript:void(0); class=theme-switch><i class="iconfont icon-sun"></i></a>&nbsp;</div></div><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></nav><nav class=navbar-mobile id=nav-mobile style=display:none><div class=container><div class=navbar-header><div><a href=javascript:void(0); class=theme-switch><i class="iconfont icon-sun"></i></a>&nbsp;<a href=https://younesbelkada.github.io/>Younes Belkada</a></div><div class=menu-toggle><span></span><span></span><span></span></div></div><div class=menu id=mobile-menu><a class=menu-item href=/posts/ title>Blog</a>
<a class=menu-item href=/notes/ title>Notes</a>
<a class=menu-item href=/categories/ title>Categories</a>
<a class=menu-item href=/projects/ title>Projects</a>
<a class=menu-item href=/about title>About</a></div></div><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></nav><main class=main><div class=container><article class=post-warp><header class=post-header><h1 class=post-title>Week 2 - RL Course</h1><div class=post-meta>Written by <a href=https://younesbelkada.github.io/ rel=author>Younes Belkada</a> with ♥
<span class=post-time>on <time datetime=2021-10-12>12 October 2021</time></span>
in
<i class="iconfont icon-folder"></i>
<span class=post-category><a href=https://younesbelkada.github.io/categories/course-notes/>Course notes</a></span>
<i class="iconfont icon-timer"></i>
3 min</div></header><div class=post-content><p>In the previous section we have seen how to formulate a RL problem using MDPs and provide some tips in order to analytically evaluate a policy (set of actions at each set) if some variables were known beforehand using the Bellman equation and the Bellman operator.</p><p>Here in the second lecture we are going to tackle the issue of how to solve an MDP (i.e get the optimal policy $\pi$) given some parameters of the problem. Few techniques exists in order to achieve this, let&rsquo;s start with the <strong>Value Iteration</strong></p><h1 id=value-iteration>Value Iteration</h1><p><figure><img src=/images/ring.svg data-sizes=auto data-src=/images/notes/rl_value_iteration.gif alt="Source of the image: https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2" class=lazyload><figcaption class=image-caption>Source of the image: https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2</figcaption></figure></p><h3 id=intuition>Intuition</h3><p>We have few tools in our toolbox. First of all, <em>how do we evaluate a policy?</em> We can simply compute the <strong>Value Function</strong> which corresponds to a kind of &ldquo;summary&rdquo; of the rewards at each state, given a policy. If we consider again the simple MDP considered last week, if the policy is $\pi = \{\textcolor{#E94D40}{a_0},\ \textcolor{#E94D40}{a_0} ,\ \textcolor{#3697DC}{a_1}\}$</p><p><figure><img src=/images/ring.svg data-sizes=auto data-src=https://cdn.mathpix.com/snip/images/rLqPQXWNtuf3BU915gjmwTnkM9O16IG2RDmc0z1iV2o.original.fullsize.png alt="Simple MDP" class=lazyload><figcaption class=image-caption>Simple MDP</figcaption></figure></p><p>Analytically, The <strong>Value Function</strong> is equal to $V^\pi = [0.033, 0.363, 0]$. Which means intuitively, that in <em>expectation</em> under the <strong>given</strong> policy, moving frequently to the state $s_1$ is highly <strong>benefic</strong> in terms of reward.</p><p><em>What if the policy is unknown?</em> Then the problem would have a different formulation, our goal would be to <strong>find the best policy possible</strong> among the combination of all possible policies. You can image that this is doable if the problem is simple enough ($|A| &lt; N_A$ and $|S| &lt; N_S$, i.e reasonable number of possible actions and reasonable number of states) but extremely complex if not.</p><p>The idea of the <strong>Value Iteration</strong> algorithm is the following:</p><ul><li>Initialize $V_0$ in $R^N$</li><li>At each iteration $k = 1,2,&mldr;,K$:<ul><li>Compute $V_{k+1} = \mathcal{T}V_k$ until $|| V_{k+1} - V_k || \leq \epsilon$</li></ul></li><li>Get the <em><strong>greedy policy</strong></em> $
\pi_{K}(s) \in \arg \max _{a \in A}\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) V_{K}\left(s^{\prime}\right)\right]
$</li></ul><p>Recall that the greedy policy is $\in$ the set of $argmax$ because we may have several values that reaches the maximum.</p><p>Intuitively, the algorithm would work as the followwing</p><ul><li>Initialize with a random <strong>Value Function</strong></li><li>Compute the Bellman operator applied to this <strong>Value Function</strong> for all <strong>the possible actions</strong> and update $V_k$ at each step by considering the action that maximized the <strong>Value Function</strong> (i.e. the optimal action)</li></ul><p>Let&rsquo;s consider the following problem, where the initial state is $S_0$ and the final state $S_3$. The set of actions here is $A = \{a_0, a_1, a_2, a_3\}$ which corresponds to the action of moving to a corresponding state. We can imagine that it is a <em>maze</em> where the agent has to start from the state 0 and tries to learn how to escape from there.</p><p><figure><img src=/images/ring.svg data-sizes=auto data-src=/images/notes/diag_rl1_value_iteration.png alt="Another simple MDP for our example" class=lazyload><figcaption class=image-caption>Another simple MDP for our example</figcaption></figure></p><p>Here the process is still a MDP because it respects the assumptions under <strong>Markov Decision Processes</strong>. We ignore in our example the transition probabilities for simplicity and apply the algorithm for one step.</p><p><figure><img src=/images/ring.svg data-sizes=auto data-src=/images/notes/rl_diag_1.gif alt="Value Iteration Algorithm for one step - Original content" class=lazyload><figcaption class=image-caption>Value Iteration Algorithm for one step - Original content</figcaption></figure></p><p>After one step, the optimal policy is the following:</p><p>$
\begin{array}{l}V_1\ =\ \left[1,\ 10,\ 10,\ 10\right]\\\<br>\pi^* =\ \left\{\left[a_1,\ a_3,\ a_3,\ a_3\right],\ \left[a_2,\ a_3,\ a_3,\ a_3\right]\right\}\end{array}
$</p><p>Which absolutely makes sense, because in this MDP, to reach an optimal reward value you should avoid moving to the state $s_0$ and move to $s_3$ whenever you can.</p><h3 id=theorem>Theorem</h3><p>Thankfully, we have a guarantee that the algorithm will give us a solution in some fixed number of steps.</p></div><div class=post-copyright><p class=copyright-item><span>Author:</span>
<span>younesbelkada</span></p><p class=copyright-item><span>Words:</span>
<span>549</span></p><p class=copyright-item><span>Share:</span>
<span><a href="//twitter.com/share?url=https%3a%2f%2fyounesbelkada.github.io%2fnotes%2frl2%2f&text=Week%202%20-%20RL%20Course&via=" target=_blank title="Share on Twitter"><i class="iconfont icon-twitter"></i></a>
<a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fyounesbelkada.github.io%2fnotes%2frl2%2f" target=_blank title="Share on Facebook"><i class="iconfont icon-facebook"></i></a>
<a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fyounesbelkada.github.io%2fnotes%2frl2%2f&title=Week%202%20-%20RL%20Course" target=_blank title="Share on LinkedIn"><i class="iconfont icon-linkedin"></i></a></span></p><p class=copyright-item>Released under <a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></div><div class=post-tags><section><a href=javascript:window.history.back();>Back</a></span> ·
<span><a href=https://younesbelkada.github.io/>Home</a></span></section></div><div class=post-nav><a href=https://younesbelkada.github.io/notes/rl1/ class=prev rel=prev title="Week 1 - RL Course"><i class="iconfont icon-dajiantou"></i>&nbsp;Week 1 - RL Course</a></div><div class=post-comment></div></article></div></main><footer class=footer><div class=copyright>&copy;
<span itemprop=copyrightYear>2021 - 2021</span>
<span class=author itemprop=copyrightHolder><a href=https://younesbelkada.github.io/>younesbelkada</a></span></div></footer><link crossorigin=anonymous integrity=sha384-yziQACfvCVwLqVFLqkWBYRO3XeA4EqzfXKGwaWnenYn5XzqfJFlFdKEmvutIQdKb href=https://lib.baomitu.com/lightgallery/1.10.0/css/lightgallery.min.css rel=stylesheet><script src=/js/vendor_gallery.min.js async></script></div></body></html>